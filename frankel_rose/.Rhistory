legend.text=element_text(size=7,face="bold"),
legend.key.height=grid::unit(0.8,"cm"),
legend.key.width=grid::unit(0.2,"cm"),
axis.line=element_blank(),
axis.text.x=element_text(size=8),
axis.text.y=element_text(size=5, vjust=0.2),
#axis.ticks=element_line(size=0.4),
axis.ticks=element_blank(),
plot.background=element_blank(),
panel.border=element_blank(),
panel.background=element_blank(),
plot.margin=margin(0.7,0.4,0.1,0.2,"cm"))
}
# write a function which can be reused!
ols2Stargazer <- function(model, title, name) {
# this gives out the basic latex file
latex <- capture.output({stargazer(model,
title = title, type = "latex",
model.numbers = T,
multicolumn = T,
intercept.bottom = T,
table.layout ="-ldc#-t-s-n",
df = FALSE, digits = 3, header = T, float = T, table.placement = "H")})
latex <- gsub("\\begin{tabular}","\\resizebox{0.4\\linewidth}{!}{\\begin{tabular}", latex, fixed=T)
latex <- gsub("\\end{tabular}","\\end{tabular}}", latex, fixed=T)
# save a copy
cat(latex, file = paste0("./tables/", name, ".tex"), sep = "\n")
return(latex)
}
calc_error_rate <- function(predicted.value, true.value)
{return(mean(true.value!=predicted.value))}
#replicating Frankel and Rose (1996)
frankel_rose_data <- read.dta("./stata/cleanrose12.dta")
make_calender <- function(df) {
df %>%
ggplot(aes(x = date, y = country, fill = factor(event))) +
geom_tile(colour="white",size=0.2) +
scale_y_discrete(expand=c(0,0)) +
scale_x_continuous(expand=c(0,0), breaks=seq(1971, 1992, 1)) +
# prefer a manual scale to make it looks better
scale_fill_manual(values=c("grey90","#d53e4f"), name = "Incidence") +
labs(title = "Incidence of Currency Crisis, 1971 - 1992", subtitle = "Currency crises identified in Frankel and Rose (1996)",
x = "", y = "") +
crisis_theme()
}
frankel_rose_crisis_calendar <- frankel_rose_data %>%
# turn country into factor so you can arrange it any way you want
mutate(country = factor(country, levels = rev(sort(unique(country)))),
event = ifelse(is.na(event), 0, event)) %>%
make_calender(.)
ggsave("./graphs/frankel_rose_crisis_calendar.pdf",
frankel_rose_crisis_calendar,
width = 297,
height = 210,
units = "mm")
realign_crisis0 <- function(df) {
country_list <- df %>% split(., .$country)
by_country <- lapply(country_list, function(country) {
crisis_dates <- country %>% filter(event == 1) %>% pull(date)
crisis <- lapply(crisis_dates, function(x){
#lower_end = x - 3
#upper_end = x + 3
df <- country %>%
#filter(date >= lower_end & date <= upper_end) %>%
mutate(crisis0 = x,
index = date - x) %>%
# get rid of data that are within 2 years after a crisis -
# this is the prevent post-crisis bias
filter(!(index >= 0 & index < 2)) %>%
# an alternative event classification
mutate(event = ifelse((index < 0 & index >= -2), 1, 0))
}) %>% bind_rows()
}) %>% bind_rows()
return(by_country)
}
frankel_rose_data_realigned <- frankel_rose_data %>%
# turn country into factor so you can arrange it any way you want
mutate(country = factor(country, levels = rev(sort(unique(country))))) %>%
realign_crisis0(.)
frankel_rose_data_realigned_calender <- frankel_rose_data_realigned %>%
select(name, country, date, event) %>%
unique(.) %>%
make_calender(.) +
labs(title = "Realigned Incidence of Currency Crisis, 1971 - 1992",
subtitle = "t-1 and t-2 of Currency crises identified in Frankel and Rose (1996)",
x = "", y = "")
ggsave("./graphs/frankel_rose_data_realigned_calender.pdf",
frankel_rose_data_realigned_calender,
width = 297,
height = 210,
units = "mm")
crisis_percent_before <- frankel_rose_data %>%
filter(!is.na(event)) %>%
mutate(total = n()) %>%
group_by(event, total) %>%
summarise(count = n()) %>%
ungroup() %>%
mutate(percent = count/total,
type = "Before")
crisis_percent_after <- frankel_rose_data_realigned %>%
mutate(total = n()) %>%
group_by(event, total) %>%
summarise(count = n()) %>%
ungroup() %>%
mutate(percent = count/total,
type = "After")
crisis_percent <- rbind(crisis_percent_before, crisis_percent_after) %>%
ggplot(aes(x = as.factor(event), y = percent)) +
geom_col(aes(fill = as.factor(event)), show.legend = F) +
facet_wrap(~type) +
labs(title = "Percentage of Crisis Before and After Realignment",
x = "Crisis", y = "Percent (%)")
ggsave("./graphs/crisis_percent.pdf",
crisis_percent,
width = 297,
height = 210,
units = "mm")
# run a simple probit model here
frankel_rose_probit <- glm(event ~ comrat + conrat + varrat + fdistock + shorttot + pubrat +
multirat + debty + reservem + cacc + defrat + dlcred + dly +
istar + overvaln, family = binomial(link = "probit"),
data = frankel_rose_data)
frankel_rose_probit_stargazed <- ols2Stargazer(model = frankel_rose_probit,
title = "Baseline probit estimates in Frankel and Rose (1996)\\label{frankel_rose_probit}",
name = "frankel_rose_probit")
records = matrix(NA, nrow = 4, ncol=2)
colnames(records) <- c("train.error", "test.error")
rownames(records) <- c("Probit", "KNN", "Random Forests", "SVM")
frankel_rose_data_ML <- frankel_rose_data_realigned %>%
mutate(event = ifelse(is.na(event), 0, event),
event = factor(event, levels = c("0", "1"), labels = c("no", "yes"))) %>%
# keep a smaller subset, simplify subsequent codes
select(event, comrat, conrat, varrat,
fdistock, shorttot, pubrat, multirat, debty,
reservem, cacc, defrat, dlcred, dly, istar, overvaln)
#frankel_rose_data_ML_cleaned <- frankel_rose_data_ML[complete.cases(frankel_rose_data_ML), ]
# generate row indices for training data
set.seed(2019)
in_train <- createDataPartition(frankel_rose_data_ML$event,
p = 0.8, list=FALSE)
training <- frankel_rose_data_ML[in_train, ]
test <- frankel_rose_data_ML[-in_train, ]
# imputing missing variables using bagged trees
# an easy tutorial can be found here:
# http://rismyhammer.com/ml/Pre-Processing.html
# covariates live in columns 2-16
preProc <- preProcess(method = "bagImpute", training[, 2:16])
training[, 2:16] <- predict(preProc, training[, 2:16])
test[, 2:16] <- predict(preProc, test[, 2:16])
probit_fit <- glm(event ~ ., family = binomial(link = "probit"), data = training)
# Training
prob_train <- predict(probit_fit, type = "response")
# Quick look at the ROC
# Do it by hand for now, can write a function later
prob_train_roc <- roc(training$event, prob_train)
# this shows the roc curve
plot(prob_train_roc, print.thres = "best", print.thres.best.method = "closest.topleft")
# look for the best top-left cutoff
prob_train_coords <- coords(prob_train_roc, "best", best.method = "closest.topleft", ret = c("threshold", "accuracy"))
# Choose the best cut-off
prob_train_labels = training %>%
mutate(predicted.value = factor(ifelse(prob_train <= prob_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
probit_train_error <- calc_error_rate(predicted.value = prob_train_labels$predicted.value, true.value = training$event)
# Test
prob_test <- predict(probit_fit, newdata = test, type = "response")
prob_test_labels = test %>%
mutate(predicted.value = factor(ifelse(prob_test <= prob_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
probit_test_error <- calc_error_rate(predicted.value = prob_test_labels$predicted.value, true.value = test$event)
# write down the training and test errors of the probit model
records[1,] <- c(probit_train_error, probit_test_error)
set.seed(2019)
cat("Before")
table(training$event)
cat("\nAfter")
rose_train <- SMOTE(event ~ ., data = training)
table(rose_train$event)
over_sampling <- trainControl(method = "cv",
number = 10,
verboseIter = F, # don't print iterations
sampling = "smote", # over-sampling technique from menardi2014training
savePredictions = "final",
classProbs=TRUE,
summaryFunction = twoClassSummary)
set.seed(2019)
knn_fit <- train(event ~ ., training,
method = "knn",
preProcess = c("center","scale"),
#metric = "ROC",
tuneLength = 15,
trControl = over_sampling)
# Training
knn_train <- predict(knn_fit, type = "prob")
# Quick look at the ROC - caret doesn't check probability threshold I think
# Do it by hand for now, can write a function later
knn_train_roc <- roc(training$event, knn_train$yes)
# this shows the roc curve
plot(knn_train_roc, print.thres = "best", print.thres.best.method = "closest.topleft")
# look for the best top-left cutoff
knn_train_coords <- coords(knn_train_roc, "best", best.method = "closest.topleft", ret = c("threshold", "accuracy"))
knn_train_labels <- training %>%
mutate(predicted.value = factor(ifelse(knn_train[,2] <= knn_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
knn_train_error <- calc_error_rate(predicted.value = knn_train_labels$predicted.value, true.value = training$event)
# Test
knn_test <- predict(knn_fit, newdata = test, type = "prob")
knn_test_labels = test %>%
mutate(predicted.value = factor(ifelse(knn_test[,2] <= knn_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
knn_test_error <- calc_error_rate(predicted.value = knn_test_labels$predicted.value, true.value = test$event)
knn_test_roc <- roc(test$event, knn_test$yes)
knn_test_coords <- coords(knn_test_roc, "best", best.method = "closest.topleft", ret = c("threshold", "accuracy"))
# write down the training and test errors of the probit model
records[2,] <- c(knn_train_error, knn_test_error)
set.seed(2019)
rf_fit <- train(event ~ ., training,
method = "ranger",
tuneLength = 15,
#metric = "ROC",
trControl = over_sampling)
# Training
rf_train <- predict(rf_fit, training, type = "prob")
# Quick look at the ROC - caret doesn't check probability threshold I think
# Do it by hand for now, can write a function later
rf_train_roc <- roc(training$event, rf_train$yes)
# this shows the roc curve
plot(rf_train_roc, print.thres = "best", print.thres.best.method = "closest.topleft")
# look for the best top-left cutoff
rf_train_coords <- coords(rf_train_roc, "best", best.method = "closest.topleft", ret = c("threshold", "accuracy"))
rf_train_labels <- training %>%
mutate(predicted.value = factor(ifelse(rf_train[,2] <= rf_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
rf_train_error <- calc_error_rate(predicted.value = rf_train_labels$predicted.value, true.value = training$event)
# Test
rf_test <- predict(rf_fit, newdata = test, type = "prob")
rf_test_labels = test %>%
mutate(predicted.value = factor(ifelse(rf_test[,2] <= rf_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
rf_test_error <- calc_error_rate(predicted.value = rf_test_labels$predicted.value, true.value = test$event)
rf_test_roc <- roc(test$event, rf_test$yes)
rf_test_coords <- coords(rf_test_roc, "best", best.method = "closest.topleft", ret = c("threshold", "accuracy"))
# write down the training and test errors of the probit model
records[3,] <- c(rf_train_error, rf_test_error)
set.seed(2019)
svm_fit <- train(event ~ ., training,
method = "svmLinear",
preProcess = c("center","scale"),
tuneLength = 15,
#metric = "ROC",
trControl = over_sampling)
# Training
svm_train <- predict(svm_fit, type = "prob")
# Quick look at the ROC - caret doesn't check probability threshold I think
# Do it by hand for now, can write a function later
svm_train_roc <- roc(training$event, svm_train$yes)
# this shows the roc curve
plot(svm_train_roc, print.thres = "best", print.thres.best.method = "closest.topleft")
# look for the best top-left cutoff
svm_train_coords <- coords(svm_train_roc, "best", best.method = "closest.topleft", ret = c("threshold", "accuracy"))
svm_train_labels <- training %>%
mutate(predicted.value = factor(ifelse(svm_train[,2] <= svm_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
svm_train_error <- calc_error_rate(predicted.value = svm_train_labels$predicted.value, true.value = training$event)
# Test
svm_test <- predict(svm_fit, newdata = test, type = "prob")
svm_test_labels = test %>%
mutate(predicted.value = factor(ifelse(svm_test[,2] <= svm_train_coords$threshold, "no", "yes"), levels = c("no", "yes")))
svm_test_error <- calc_error_rate(predicted.value = svm_test_labels$predicted.value, true.value = test$event)
svm_test_roc <- roc(test$event, svm_test$yes)
svm_test_coords <- coords(svm_test_roc, "best", best.method = "closest.topleft", ret = c("threshold", "accuracy"))
svm_test_error <- calc_error_rate(predicted.value = svm_test_labels$predicted.value, true.value = test$event)
# write down the training and test errors of the probit model
records[4,] <- c(svm_train_error, svm_test_error)
records
#ROC on test data
# probit
pred_prob <- prediction(prob_test, test$event)
performance_prob <- performance(pred_prob, measure = "tpr", x.measure="fpr")
# knn
pred_knn <- prediction(knn_test[,2], test$event)
performance_knn <- performance(pred_knn, measure = "tpr", x.measure="fpr")
# rf
pred_rf <- prediction(rf_test[,2], test$event)
performance_rf <- performance(pred_rf, measure = "tpr", x.measure="fpr")
# svm
pred_svm <- prediction(svm_test[,2], test$event)
performance_svm <- performance(pred_svm, measure = "tpr", x.measure="fpr")
#plot
plot(performance_prob, col = 2, lwd = 2, main = "ROC Curves for These Two Classification Methods")
legend(0.6, 0.6, c("Probit", "KNN", "Random Forests", "SVM"), 2:5)
#others
plot(performance_knn,col=3,lwd=2,add=TRUE)
plot(performance_rf,col=4,lwd=2,add=TRUE)
plot(performance_svm,col=5,lwd=2,add=TRUE)
abline(0,1)
Area_Under_the_Curve = matrix(NA, nrow=4, ncol=1)
colnames(Area_Under_the_Curve) <- c("AUC")
rownames(Area_Under_the_Curve) <- c("Probit", "KNN", "Random Forests", "SVM")
auc_probit <-performance(pred_prob, "auc")@y.values
Area_Under_the_Curve[1,] <-c(as.numeric(auc_probit))
auc_knn <- performance(pred_knn,"auc")@y.values
Area_Under_the_Curve[2,] <- c(as.numeric(auc_knn))
auc_rf <- performance(pred_rf,"auc")@y.values
Area_Under_the_Curve[3,] <- c(as.numeric(auc_rf))
auc_svm <- performance(pred_svm,"auc")@y.values[[1]]
Area_Under_the_Curve[4,] <- c(as.numeric(auc_svm))
Area_Under_the_Curve
confusion <- lapply(list(prob_test_labels, knn_test_labels, rf_test_labels, svm_test_labels), function(x) {
confusionMatrix(x$predicted.value, x$event)
})
make_confusion_kable <- function(cm) {
table = cm$table
colnames(table) = paste0("Actual = ", colnames(table))
rownames(table) = paste0("Prediction = ", rownames(table))
return(table)
}
tidied_tables <- lapply(confusion, make_confusion_kable)
# switching it because the way our table is made
sensitivity <- lapply(confusion, function(x) {data.frame("Sensitivity" = as.numeric(x$byClass[2]))}) %>% bind_rows()
specificity <- lapply(confusion, function(x) {data.frame("Specificity" = as.numeric(x$byClass[1]))}) %>% bind_rows()
main_metrics <- data.frame("Model" = c("probit", "knn", "random forest", "svm"),
sensitivity, specificity)
main_metrics_plot <- main_metrics %>% melt(id.var = "Model") %>%
ggplot(aes(x = Model, y = value)) +
geom_col(aes(fill = Model)) +
facet_wrap(~variable) +
labs(title = "Model trade-offs: Sensitivity = True Positive Rate,\nSpecificity = 1 - (False Alarm Rate)")
main_metrics_plot
tidied_tables[[1]]
tidied_tables[[2]]
tidied_tables[[3]]
tidied_tables[[4]]
Area_Under_the_Curve
cat("\nAfter"; rose_train$event)
cat("\nAfter", rose_train$event)
cat("\nAfter", table(rose_train$event)
cat("\nAfter", table(rose_train$event))
cat("\nAfter", table(rose_train$event))
set.seed(2019)
smote_train <- SMOTE(event ~ ., data = training)
table(smote_train$event)
340+82
340/422
library(dplyr)
library(ggplot2)
library(countrycode)
library(imfr)
library(foreign)
library(ggthemr)
library(stargazer)
library(summarytools)
library(knitr)
library(caret)
library(kableExtra)
library(RANN)
library(reshape2)
library(taRifx)
library(ROCR)
library(DMwR)
library(pROC)
library(ROSE)
ggthemr("flat")
crisis_theme <- function(){
theme(legend.position="right",legend.direction="vertical",
legend.margin=margin(grid::unit(0,"cm")),
legend.text=element_text(size=7,face="bold"),
legend.key.height=grid::unit(0.8,"cm"),
legend.key.width=grid::unit(0.2,"cm"),
axis.line=element_blank(),
axis.text.x=element_text(size=8),
axis.text.y=element_text(size=5, vjust=0.2),
#axis.ticks=element_line(size=0.4),
axis.ticks=element_blank(),
plot.background=element_blank(),
panel.border=element_blank(),
panel.background=element_blank(),
plot.margin=margin(0.7,0.4,0.1,0.2,"cm"))
}
# write a function which can be reused!
ols2Stargazer <- function(model, title, name) {
# this gives out the basic latex file
latex <- capture.output({stargazer(model,
title = title, type = "latex",
model.numbers = T,
multicolumn = T,
intercept.bottom = T,
table.layout ="-ldc#-t-s-n",
df = FALSE, digits = 3, header = T, float = T, table.placement = "H")})
latex <- gsub("\\begin{tabular}","\\resizebox{0.4\\linewidth}{!}{\\begin{tabular}", latex, fixed=T)
latex <- gsub("\\end{tabular}","\\end{tabular}}", latex, fixed=T)
# save a copy
cat(latex, file = paste0("./tables/", name, ".tex"), sep = "\n")
return(latex)
}
calc_error_rate <- function(predicted.value, true.value)
{return(mean(true.value!=predicted.value))}
#replicating Frankel and Rose (1996)
frankel_rose_data <- read.dta("./stata/cleanrose12.dta")
write.csv(frankel_rose_data, "frankel_rose_data.csv")
frankel_rose_data_ML <- frankel_rose_data_realigned %>%
mutate(event = ifelse(is.na(event), 0, event),
event = factor(event, levels = c("0", "1"), labels = c("no", "yes"))) %>%
# keep a smaller subset, simplify subsequent codes
select(event, comrat, conrat, varrat,
fdistock, shorttot, pubrat, multirat, debty,
reservem, cacc, defrat, dlcred, dly, istar, overvaln)
realign_crisis0 <- function(df) {
country_list <- df %>% split(., .$country)
by_country <- lapply(country_list, function(country) {
crisis_dates <- country %>% filter(event == 1) %>% pull(date)
crisis <- lapply(crisis_dates, function(x){
#lower_end = x - 3
#upper_end = x + 3
df <- country %>%
#filter(date >= lower_end & date <= upper_end) %>%
mutate(crisis0 = x,
index = date - x) %>%
# get rid of data that are within 2 years after a crisis -
# this is the prevent post-crisis bias
filter(!(index >= 0 & index < 2)) %>%
# an alternative event classification
mutate(event = ifelse((index < 0 & index >= -2), 1, 0))
}) %>% bind_rows()
}) %>% bind_rows()
return(by_country)
}
frankel_rose_data_realigned <- frankel_rose_data %>%
# turn country into factor so you can arrange it any way you want
mutate(country = factor(country, levels = rev(sort(unique(country))))) %>%
realign_crisis0(.)
frankel_rose_data_realigned_calender <- frankel_rose_data_realigned %>%
select(name, country, date, event) %>%
unique(.) %>%
make_calender(.) +
labs(title = "Realigned Incidence of Currency Crisis, 1971 - 1992",
subtitle = "t-1 and t-2 of Currency crises identified in Frankel and Rose (1996)",
x = "", y = "")
make_calender <- function(df) {
df %>%
ggplot(aes(x = date, y = country, fill = factor(event))) +
geom_tile(colour="white",size=0.2) +
scale_y_discrete(expand=c(0,0)) +
scale_x_continuous(expand=c(0,0), breaks=seq(1971, 1992, 1)) +
# prefer a manual scale to make it looks better
scale_fill_manual(values=c("grey90","#d53e4f"), name = "Incidence") +
labs(title = "Incidence of Currency Crisis, 1971 - 1992", subtitle = "Currency crises identified in Frankel and Rose (1996)",
x = "", y = "") +
crisis_theme()
}
frankel_rose_crisis_calendar <- frankel_rose_data %>%
# turn country into factor so you can arrange it any way you want
mutate(country = factor(country, levels = rev(sort(unique(country)))),
event = ifelse(is.na(event), 0, event)) %>%
make_calender(.)
ggsave("./graphs/frankel_rose_crisis_calendar.pdf",
frankel_rose_crisis_calendar,
width = 297,
height = 210,
units = "mm")
realign_crisis0 <- function(df) {
country_list <- df %>% split(., .$country)
by_country <- lapply(country_list, function(country) {
crisis_dates <- country %>% filter(event == 1) %>% pull(date)
crisis <- lapply(crisis_dates, function(x){
#lower_end = x - 3
#upper_end = x + 3
df <- country %>%
#filter(date >= lower_end & date <= upper_end) %>%
mutate(crisis0 = x,
index = date - x) %>%
# get rid of data that are within 2 years after a crisis -
# this is the prevent post-crisis bias
filter(!(index >= 0 & index < 2)) %>%
# an alternative event classification
mutate(event = ifelse((index < 0 & index >= -2), 1, 0))
}) %>% bind_rows()
}) %>% bind_rows()
return(by_country)
}
frankel_rose_data_realigned <- frankel_rose_data %>%
# turn country into factor so you can arrange it any way you want
mutate(country = factor(country, levels = rev(sort(unique(country))))) %>%
realign_crisis0(.)
frankel_rose_data_realigned_calender <- frankel_rose_data_realigned %>%
select(name, country, date, event) %>%
unique(.) %>%
make_calender(.) +
labs(title = "Realigned Incidence of Currency Crisis, 1971 - 1992",
subtitle = "t-1 and t-2 of Currency crises identified in Frankel and Rose (1996)",
x = "", y = "")
ggsave("./graphs/frankel_rose_data_realigned_calender.pdf",
frankel_rose_data_realigned_calender,
width = 297,
height = 210,
units = "mm")
frankel_rose_data_ML <- frankel_rose_data_realigned %>%
mutate(event = ifelse(is.na(event), 0, event),
event = factor(event, levels = c("0", "1"), labels = c("no", "yes"))) %>%
# keep a smaller subset, simplify subsequent codes
select(event, comrat, conrat, varrat,
fdistock, shorttot, pubrat, multirat, debty,
reservem, cacc, defrat, dlcred, dly, istar, overvaln)
#frankel_rose_data_ML_cleaned <- frankel_rose_data_ML[complete.cases(frankel_rose_data_ML), ]
# generate row indices for training data
set.seed(2019)
in_train <- createDataPartition(frankel_rose_data_ML$event,
p = 0.8, list=FALSE)
training <- frankel_rose_data_ML[in_train, ]
test <- frankel_rose_data_ML[-in_train, ]
# imputing missing variables using bagged trees
# an easy tutorial can be found here:
# http://rismyhammer.com/ml/Pre-Processing.html
# covariates live in columns 2-16
preProc <- preProcess(method = "bagImpute", training[, 2:16])
training[, 2:16] <- predict(preProc, training[, 2:16])
test[, 2:16] <- predict(preProc, test[, 2:16])
write.csv(training, "training.csv")
write.csv(test, "test.csv")
